name: Deploy LLM Services Only

on:
  push:
    branches:
      - main
      - develop
    paths:
      - 'llms/**'
      - '.github/workflows/deploy-llm.yml'
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_PREFIX: ${{ github.repository_owner }}

jobs:
  build-llm-services:
    name: Build LLM Service Images
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    strategy:
      matrix:
        service:
          - name: ai-service
            context: ./llms/ai
            dockerfile: ./llms/ai/Dockerfile
          - name: gpu-monitor
            context: ./llms/gpu_monitor
            dockerfile: ./llms/gpu_monitor/Dockerfile
          - name: model-initializer
            context: ./llms/initializer
            dockerfile: ./llms/initializer/Dockerfile

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Free Disk Space (only for large ai-service build)
        if: matrix.service.name == 'ai-service'
        run: |
          echo "=== Before cleanup ==="
          df -h
          sudo rm -rf /usr/share/dotnet /usr/local/lib/android /opt/ghc /opt/hostedtoolcache/CodeQL
          sudo docker image prune -af
          sudo docker system prune -af --volumes
          echo "=== After cleanup ==="
          df -h

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_PREFIX }}/llm-${{ matrix.service.name }}
          tags: |
            type=ref,event=branch
            type=sha,format=long,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: ${{ matrix.service.context }}
          file: ${{ matrix.service.dockerfile }}
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha,scope=${{ matrix.service.name }}
          cache-to: type=gha,mode=max,scope=${{ matrix.service.name }}
          build-args: |
            BUILDKIT_INLINE_CACHE=1

  deploy-llm-servers:
    name: Deploy to LLM Servers (1.61 & 1.62)
    runs-on: ubuntu-latest
    needs: [build-llm-services]

    strategy:
      matrix:
        server:
          - host: 192.168.1.61
            name: llm-server-1
          - host: 192.168.1.62
            name: llm-server-2

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup SSH key for central server
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -p 2324 217.219.39.212 >> ~/.ssh/known_hosts

      - name: Deploy to ${{ matrix.server.name }} via central server
        run: |
          ssh -p 2324 -i ~/.ssh/deploy_key ${{ secrets.SSH_USER }}@217.219.39.212 << 'ENDSSH'
          set -euo pipefail

          echo "=== Deploying to ${{ matrix.server.name }} (${{ matrix.server.host }}) ==="

          # Test SSH to target server
          ssh -o StrictHostKeyChecking=no ubuntu@${{ matrix.server.host }} "echo 'SSH OK'"

          # Prepare directories
          ssh ubuntu@${{ matrix.server.host }} "mkdir -p ~/llm-deployment ~/models/huggingface"

          # Clone repo temporarily on central server
          cd /tmp
          rm -rf llm-deploy-temp
          git clone --depth 1 --branch ${{ github.ref_name }} https://github.com/${{ github.repository }}.git llm-deploy-temp

          # Copy deployment files
          scp -r /tmp/llm-deploy-temp/llms/* ubuntu@${{ matrix.server.host }}:~/llm-deployment/

          # Create .env with exact tag
          ssh ubuntu@${{ matrix.server.host }} << 'ENVEOF'
          cat > ~/llm-deployment/.env << EOF
          GITHUB_REGISTRY=${{ env.REGISTRY }}
          GITHUB_REPOSITORY_OWNER=${{ env.IMAGE_PREFIX }}
          IMAGE_TAG=${{ github.ref_name }}-${{ github.sha }}
          MODEL_CACHE_PATH=/home/ubuntu/models/huggingface
          EOF
          ENVEOF

          echo "=== .env on created on remote ==="
          ssh ubuntu@${{ matrix.server.host }} "cat ~/llm-deployment/.env"

          # Docker login
          echo "=== Logging into GHCR ==="
          ssh ubuntu@${{ matrix.server.host }} "echo '${{ secrets.GITHUB_TOKEN }}' | docker login ${{ env.REGISTRY }} -u ${{ github.actor }} --password-stdin"

          # Stop old containers
          echo "=== Stopping old services ==="
          ssh ubuntu@${{ matrix.server.host }} "cd ~/llm-deployment && docker compose down"

          # Prune old images
          ssh ubuntu@${{ matrix.server.host }} "docker image prune -f"

          # Pull new images
          echo "=== Pulling images (tag: ${{ github.ref_name }}-${{ github.sha }}) ==="
          ssh ubuntu@${{ matrix.server.host }} "cd ~/llm-deployment && docker compose pull"
          ssh ubuntu@${{ matrix.server.host }} "docker images | grep llm- || echo 'No llm- images found after pull'"

          # Start services
          echo "=== Starting services ==="
          ssh ubuntu@${{ matrix.server.host }} "cd ~/llm-deployment && docker compose up -d"
          ssh ubuntu@${{ matrix.server.host }} "cd ~/llm-deployment && docker compose ps"

          # Wait for model_initializer
          echo "=== Waiting for model_initializer to finish ==="
          ssh ubuntu@${{ matrix.server.host }} "
            timeout 1800 bash -c '
              while docker ps -a --format \"{{.Name}}\" | grep -q model_initializer; do
                status=\$(docker inspect --format='{{.State.Status}}' model_initializer 2>/dev/null || echo 'missing')
                if [ \"\$status\" = \"exited\" ]; then
                  code=\$(docker inspect --format='{{.State.ExitCode}}' model_initializer)
                  if [ \"\$code\" = \"0\" ]; then
                    echo \"model_initializer completed successfully\"
                    break
                  else
                    echo \"model_initializer failed (exit \$code)\"
                    docker logs model_initializer --tail 50
                    exit 1
                  fi
                fi
                echo \"Waiting for initializer... (\$status)\"
                sleep 10
              done
            '
          "

          # Wait for ai_service healthy
          echo "=== Waiting for ai_service to become healthy ==="
          ssh ubuntu@${{ matrix.server.host }} "
            cd ~/llm-deployment
            timeout 300 bash -c '
              until docker compose ps ai_service | grep -q healthy; do
                echo \"ai_service not healthy yet...\"
                sleep 10
              done
              echo \"ai_service is healthy!\"
            '
          "

          # Final status
          ssh ubuntu@${{ matrix.server.host }} "cd ~/llm-deployment && docker compose ps"
          ssh ubuntu@${{ matrix.server.host }} "nvidia-smi || true"

          # Cleanup
          rm -rf /tmp/llm-deploy-temp
          echo "=== Deployment completed successfully on ${{ matrix.server.name }} ==="
          ENDSSH

      - name: Show detailed logs on failure
        if: failure()
        run: |
          ssh -p 2324 -i ~/.ssh/deploy_key ${{ secrets.SSH_USER }}@217.219.39.212 << 'ENDSSH'
          echo "=== FAILURE LOGS FOR ${{ matrix.server.host }} ==="
          ssh ubuntu@${{ matrix.server.host }} "
            cd ~/llm-deployment || exit 0
            echo '=== docker compose ps ==='
            docker compose ps -a
            echo
            echo '=== model_initializer logs ==='
            docker logs model_initializer --tail 100 2>/dev/null || echo 'not found'
            echo
            echo '=== ai_service logs ==='
            docker logs ai_service --tail 100 2>/dev/null || echo 'not found'
            echo
            echo '=== gpu_monitor logs ==='
            docker logs gpu_monitor --tail 50 2>/dev/null || echo 'not found'
          "
          ENDSSH

  health-check:
    name: LLM Services Health Check
    runs-on: ubuntu-latest
    needs: [deploy-llm-servers]

    steps:
      - name: Setup SSH key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.SSH_PRIVATE_KEY }}" > ~/.ssh/deploy_key
          chmod 600 ~/.ssh/deploy_key
          ssh-keyscan -p 2324 217.219.39.212 >> ~/.ssh/known_hosts

      - name: Perform health checks
        run: |
          ssh -p 2324 -i ~/.ssh/deploy_key ${{ secrets.SSH_USER }}@217.219.39.212 << 'ENDSSH'
          set -euo pipefail

          echo "=== Health check 192.168.1.61 ==="
          curl -f --max-time 15 http://192.168.1.61/health && echo "1.61 OK"

          echo "=== Health check 192.168.1.62 ==="
          curl -f --max-time 15 http://192.168.1.62/health && echo "1.62 OK"

          echo "==================================================================="
          echo "LLM SERVICES ARE UP AND HEALTHY"
          echo "Server 1 → http://192.168.1.61"
          echo "Server 2 → http://192.168.1.62"
          echo "Tag used → ${{ github.ref_name }}-${{ github.sha }}"
          echo "==================================================================="
          ENDSSH