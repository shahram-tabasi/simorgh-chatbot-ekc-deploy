services:
  # Model Initializer - Runs once to download models
  model_initializer:
    image: ${GITHUB_REGISTRY}/${GITHUB_REPOSITORY_OWNER}/llm-model-initializer:${IMAGE_TAG:-latest}
    container_name: model_initializer
    volumes:
      # Mount host directory where models will be stored
      - ${MODEL_CACHE_PATH:-/home/ubuntu/models/huggingface}:/models/huggingface
    environment:
      - PYTHONUNBUFFERED=1
    networks:
      - ai_net
    restart: "no" # Run once and exit

  # AI Service (Local LLM) - Depends on initializer
  ai_service:
    image: ${GITHUB_REGISTRY}/${GITHUB_REPOSITORY_OWNER}/llm-ai-service:${IMAGE_TAG:-latest}
    container_name: ai_service
    depends_on:
      model_initializer:
        condition: service_completed_successfully
    environment:
      - PYTHONUNBUFFERED=1
      - CUDA_VISIBLE_DEVICES=0
    expose:
      - "9000"
    volumes:
      # Mount the same model cache (read-only for safety)
      - ${MODEL_CACHE_PATH:-/home/ubuntu/models/huggingface}:/root/.cache/huggingface/hub:ro
    networks:
      - ai_net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    healthcheck:
      test: [ "CMD-SHELL", "wget -qO- http://127.0.0.1:9000/health | grep -q 'healthy' || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s # Reduced - model is pre-downloaded
    restart: unless-stopped

  # Nginx for secure routing
  nginx:
    image: nginx:alpine
    container_name: nginx_ai
    depends_on:
      ai_service:
        condition: service_healthy
    ports:
      - "80:80"
    environment:
      - TZ=Asia/Tehran
    volumes:
      - ./nginx_configs/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx_configs/conf.d/default.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - ai_net
    restart: unless-stopped

  # GPU Monitor (Optional but recommended)
  gpu_monitor:
    image: ${GITHUB_REGISTRY}/${GITHUB_REPOSITORY_OWNER}/llm-gpu-monitor:${IMAGE_TAG:-latest}
    container_name: gpu_monitor
    environment:
      - GPU_MEMORY_THRESHOLD=90
      - GPU_IDLE_THRESHOLD=5
      - IDLE_DURATION=60
      - CHECK_INTERVAL=10
      - TARGET_CONTAINER=ai_service
      - BACKEND_URL=http://ai_service:9000
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ utility ]
    networks:
      - ai_net
    depends_on:
      ai_service:
        condition: service_healthy
    restart: unless-stopped

networks:
  ai_net:
    driver: bridge
