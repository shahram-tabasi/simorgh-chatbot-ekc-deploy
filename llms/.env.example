# Docker Compose Environment Variables
# Copy this file to .env and configure for your environment

# ============================================================================
# Registry Configuration
# ============================================================================
GITHUB_REGISTRY=ghcr.io
GITHUB_REPOSITORY_OWNER=shahram-tabasi
IMAGE_TAG=latest

# ============================================================================
# Model Storage Configuration
# ============================================================================
# Path on host machine where models will be downloaded and cached
# This directory should have sufficient space (~40GB+ for both models)
MODEL_CACHE_PATH=/home/ubuntu/models

# ============================================================================
# Hugging Face Configuration
# ============================================================================
# HuggingFace API token (required for gated models)
# Get your token from: https://huggingface.co/settings/tokens
# Leave empty if model doesn't require authentication
HF_TOKEN=

# ============================================================================
# LangChain Tool Configuration
# ============================================================================
# Enable web search tool (uses DuckDuckGo by default)
ENABLE_SEARCH_TOOL=true

# Enable Python REPL tool (SECURITY WARNING: allows code execution)
# Only enable in trusted environments with proper sandboxing
ENABLE_PYTHON_REPL=false

# Enable verbose logging for agent reasoning steps
AGENT_VERBOSE=false

# ============================================================================
# Self-Hosted Search API (Optional)
# ============================================================================
# If you have a self-hosted search API (e.g., serpapi-bing)
# Leave empty to use DuckDuckGo
SEARCH_API_URL=
SEARCH_API_KEY=

# ============================================================================
# Python REPL Configuration (if enabled)
# ============================================================================
# Timeout for Python code execution (seconds)
PYTHON_REPL_TIMEOUT=10

# Use subprocess sandboxing (recommended)
PYTHON_REPL_SANDBOX=true

# ============================================================================
# Model Configuration
# ============================================================================
# Path to 16-bit model (inside container)
# This should match the local_dir in initializer config
MODEL_PATH=/models/unsloth-gpt-oss-20b-16bit

# ============================================================================
# Directory Structure
# ============================================================================
# The MODEL_CACHE_PATH will contain:
# - huggingface/                        (HF cache for 4-bit model)
# - unsloth-gpt-oss-20b-16bit/         (16-bit model for vLLM)
# - .init_complete                      (marker file from initializer)
#
# Example layout:
# /home/ubuntu/models/
# ├── huggingface/
# │   └── models--unsloth--gpt-oss-20b-unsloth-bnb-4bit/
# ├── unsloth-gpt-oss-20b-16bit/
# │   ├── config.json
# │   ├── model.safetensors
# │   └── ...
# └── .init_complete
