# Model Initializer Configuration
# Add models that need to be downloaded before services start

models:
  # 4-bit quantized model (fallback) - Already downloaded
  - name: "unsloth/gpt-oss-20b-unsloth-bnb-4bit"
    type: "huggingface"
    cache_dir: "/models/huggingface"
    local_dir: "/models/huggingface/unsloth--gpt-oss-20b-unsloth-bnb-4bit"
    description: "4-bit quantized model for fallback (existing download)"

  # 16-bit model - Already exists, no download needed (app.py loads it directly)
  # The model at /models/unsloth-gpt-oss-20b-16bit is already complete and ready to use

  # Add more models as needed
  # - name: "another/model"
  #   type: "huggingface"
  #   cache_dir: "/models/huggingface"