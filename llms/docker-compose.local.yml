# =============================================================================
# LLM Services - Docker Compose (LOCAL BUILD)
# Server: GPU Server
#
# This version builds all images locally from Dockerfiles instead of pulling
# from GitHub Container Registry. Use this when GitHub network is unavailable.
#
# Usage: docker-compose -f docker-compose.local.yml up -d --build
# =============================================================================

services:
  # ===========================================================================
  # MODEL INITIALIZER - Downloads and prepares models (LOCAL BUILD)
  # ===========================================================================
  model_initializer:
    build:
      context: ./initializer
      dockerfile: Dockerfile
    image: llm-model-initializer:local
    container_name: model_initializer
    volumes:
      # Mount host directory where models will be stored
      - ${MODEL_CACHE_PATH:-/home/ubuntu/models}:/models
    environment:
      - PYTHONUNBUFFERED=1
      - HF_TOKEN=${HF_TOKEN:-}
    networks:
      - ai_net
    restart: "no"  # Run once and exit

  # ===========================================================================
  # AI SERVICE - Local LLM with vLLM (LOCAL BUILD)
  # ===========================================================================
  ai_service:
    build:
      context: ./ai
      dockerfile: Dockerfile
    image: llm-ai-service:local
    container_name: ai_service
    depends_on:
      model_initializer:
        condition: service_completed_successfully
    environment:
      - PYTHONUNBUFFERED=1
      - CUDA_VISIBLE_DEVICES=0

      # vLLM and model configuration
      - MODEL_PATH=/models/unsloth-gpt-oss-20b-16bit
      - HF_TOKEN=${HF_TOKEN:-}

      # LangChain tool configuration
      - ENABLE_SEARCH_TOOL=true
      - ENABLE_PYTHON_REPL=false
      - AGENT_VERBOSE=false

      # Optional: Self-hosted search API
      - SEARCH_API_URL=${SEARCH_API_URL:-}
      - SEARCH_API_KEY=${SEARCH_API_KEY:-}

      # Python REPL configuration
      - PYTHON_REPL_TIMEOUT=10
      - PYTHON_REPL_SANDBOX=true
    expose:
      - "9000"
    volumes:
      # Mount models directory (read-only for safety)
      - ${MODEL_CACHE_PATH:-/home/ubuntu/models}:/models:ro
      # HF cache for vLLM tokenizer/metadata
      - ${MODEL_CACHE_PATH:-/home/ubuntu/models}/huggingface:/root/.cache/huggingface/hub
    networks:
      - ai_net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
    healthcheck:
      test: [ "CMD-SHELL", "wget -qO- http://127.0.0.1:9000/health | grep -q 'healthy' || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 240s  # 4 minutes for model loading
    restart: unless-stopped

  # ===========================================================================
  # NGINX - Reverse Proxy for AI Service
  # ===========================================================================
  nginx:
    image: nginx:alpine
    container_name: nginx_ai
    depends_on:
      ai_service:
        condition: service_healthy
    ports:
      - "80:80"
    environment:
      - TZ=Asia/Tehran
    volumes:
      - ./nginx_configs/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx_configs/conf.d/default.conf:/etc/nginx/conf.d/default.conf:ro
    networks:
      - ai_net
    restart: unless-stopped

  # ===========================================================================
  # GPU MONITOR - GPU Health Monitoring (LOCAL BUILD)
  # ===========================================================================
  gpu_monitor:
    build:
      context: ./gpu_monitor
      dockerfile: Dockerfile
    image: llm-gpu-monitor:local
    container_name: gpu_monitor
    environment:
      - GPU_MEMORY_THRESHOLD=90
      - GPU_IDLE_THRESHOLD=5
      - IDLE_DURATION=60
      - CHECK_INTERVAL=10
      - TARGET_CONTAINER=ai_service
      - BACKEND_URL=http://ai_service:9000
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ utility ]
    networks:
      - ai_net
    depends_on:
      ai_service:
        condition: service_healthy
    restart: unless-stopped

# =============================================================================
# NETWORKS
# =============================================================================
networks:
  ai_net:
    driver: bridge
