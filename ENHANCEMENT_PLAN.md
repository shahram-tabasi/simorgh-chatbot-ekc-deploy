# Simorgh Chatbot Enhancement Plan
## Deep Code Mining Analysis & Implementation Roadmap

Based on comprehensive code analysis, this document outlines high-impact enhancements to fully utilize existing but disconnected features.

---

## üéØ EXECUTIVE SUMMARY

**Key Findings:**
- ‚úÖ **60+ extraction guides** defined but underutilized
- ‚úÖ **Enhanced section extraction** exists but not in main flow
- ‚úÖ **Graph entity extraction** works but limited to background tasks
- ‚úÖ **CocoIndex integration** separate from main pipeline
- ‚úÖ **Graph query endpoints** exist but not exposed to frontend
- ‚ùå **No anthology/knowledge base** actually integrated

**Impact:** Main document upload uses basic chunking while sophisticated features sit unused.

---

## üìä ENHANCEMENT PRIORITIES

| Priority | Enhancement | Effort | Impact | ROI |
|----------|-------------|--------|--------|-----|
| üî¥ **P0** | Integrate enhanced pipeline into main upload | High | Very High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| üü† **P1** | Initialize extraction guides on project creation | Low | High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| üü† **P1** | Expose graph query endpoints to frontend | Medium | High | ‚≠ê‚≠ê‚≠ê‚≠ê |
| üü° **P2** | Integrate CocoIndex into enhanced pipeline | Medium | Medium | ‚≠ê‚≠ê‚≠ê |
| üü° **P2** | Add electrical knowledge base to LLM prompts | Medium | Medium | ‚≠ê‚≠ê‚≠ê |
| üü¢ **P3** | Create spec review interface | High | Medium | ‚≠ê‚≠ê |

---

## üî¥ P0: INTEGRATE ENHANCED PIPELINE INTO MAIN UPLOAD

### Current Problem
```python
# routes/documents_rag.py (line 143-160)
# Only uses basic chunking for general docs
if vector_rag:
    index_result = await vector_rag.index_document(
        markdown_content=markdown_content,  # ‚ùå Whole document, no sections
        user_id=user_id,
        filename=file_path.name,
        doc_id=doc_id
    )
```

### Solution: Replace with Enhanced Pipeline

**File:** `/simorgh-agent/backend/routes/documents_rag.py`

**Changes:**

1. **Add imports** (after line 21):
```python
from services.section_retriever import SectionRetriever
from services.document_overview_service import DocumentOverviewService
from services.graph_builder import GraphBuilder
from services.guide_executor import GuideExecutor
from services.qdrant_service import QdrantService
from services.llm_service import LLMService
```

2. **Modify dependency injection** (line 185):
```python
@router.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    user_id: str = Form(...),
    project_oenum: Optional[str] = Form(None),
    current_user: str = Depends(get_current_user),
    neo4j: Neo4jService = Depends(get_neo4j),
    redis: RedisService = Depends(get_redis),
    llm: LLMService = Depends(get_llm)  # ‚úÖ ADD THIS
):
```

3. **Replace process_and_index_document function** (line 83-178):
```python
async def process_and_index_document_enhanced(
    file_path: Path,
    user_id: str,
    project_oenum: Optional[str],
    doc_processor: DocProcessorClient,
    classifier: DocumentClassifier,
    vector_rag: Optional[VectorRAG],
    graph_init: Optional[ProjectGraphInitializer],
    neo4j_service: Neo4jService,
    redis_service: RedisService,
    llm_service: LLMService
) -> Dict[str, Any]:
    """
    ENHANCED: Process document with section extraction, summarization, and entity extraction
    """
    try:
        # 1. Process to markdown (same as before)
        logger.info(f"üìÑ Processing document: {file_path.name}")
        result = await doc_processor.process_document(file_path, user_id)

        if not result.get('success'):
            raise Exception(f"Document processing failed: {result.get('error')}")

        markdown_content = result['content']
        output_path = result.get('output_path')

        # 2. Classify document (same as before)
        logger.info(f"üîç Classifying document: {file_path.name}")
        category, doc_type, confidence = classifier.classify(
            filename=file_path.name,
            content=markdown_content
        )

        doc_id = str(uuid.uuid4())

        # ============================================================
        # ‚úÖ ENHANCED: Use Section-based Processing for ALL documents
        # ============================================================
        logger.info(f"üöÄ Starting enhanced document processing")

        # Initialize enhanced services
        qdrant = QdrantService()
        section_retriever = SectionRetriever(
            llm_service=llm_service,
            qdrant_service=qdrant
        )
        doc_overview = DocumentOverviewService(redis_service=redis_service)

        # Extract sections + generate summaries + store in Qdrant
        processing_result = section_retriever.process_and_store_document(
            markdown_content=markdown_content,
            project_number=project_oenum or "general",
            document_id=doc_id,
            filename=file_path.name,
            document_type_hint=f"{doc_type} Document",
            llm_mode="offline"  # Use local LLM (or make configurable)
        )

        if not processing_result.get("success"):
            logger.warning(f"‚ö†Ô∏è Enhanced processing failed, falling back to basic indexing")
            # Fallback to old method if enhanced fails
            if vector_rag and not project_oenum:
                index_result = await vector_rag.index_document(
                    markdown_content=markdown_content,
                    user_id=user_id,
                    filename=file_path.name,
                    doc_id=doc_id
                )
                chunks_indexed = index_result.get('chunks_indexed')
            else:
                chunks_indexed = 0
        else:
            chunks_indexed = processing_result.get("sections_extracted", 0)
            logger.info(f"‚úÖ Enhanced processing complete: {chunks_indexed} sections")

            # Track document in overview
            summary_stats = processing_result.get("summary_stats", {})
            doc_overview.add_document(
                document_id=doc_id,
                filename=file_path.name,
                document_type=doc_type,
                category=category,
                key_topics=summary_stats.get("total_subjects", 0),
                sections_count=chunks_indexed,
                total_chars=len(markdown_content),
                project_number=project_oenum,
                user_id=user_id
            )

        # 3. Add to graph structure if project document
        if project_oenum and graph_init:
            logger.info(f"üìä Adding to project graph: {project_oenum}")

            success = graph_init.add_document_to_structure(
                project_oenum=project_oenum,
                category=category,
                doc_type=doc_type,
                document_id=doc_id,
                document_metadata={
                    'filename': file_path.name,
                    'doc_type': doc_type,
                    'category': category,
                    'confidence': confidence,
                    'markdown_path': output_path,
                    'uploaded_by': user_id,
                    'sections_extracted': chunks_indexed
                }
            )

            # ============================================================
            # ‚úÖ ENHANCED: Extract entities for ALL project documents
            # ============================================================
            if success and doc_type in ["Spec", "Drawing", "Technical"]:
                logger.info(f"üîç Extracting entities from {doc_type} document")

                graph_builder = GraphBuilder(neo4j_service.driver)
                entity_result = graph_builder.extract_entities_from_spec(
                    project_oenum=project_oenum,
                    document_id=doc_id,
                    spec_content=markdown_content,
                    filename=file_path.name,
                    llm_mode="offline"
                )

                if entity_result.get("success"):
                    logger.info(f"‚úÖ Extracted {len(entity_result.get('entities', []))} entities")

                # ‚úÖ ENHANCED: Execute extraction guides for Spec documents
                if doc_type == "Spec" and category == "Client":
                    logger.info(f"üìã Executing extraction guides for spec document")

                    guide_executor = GuideExecutor(
                        neo4j_driver=neo4j_service.driver,
                        qdrant_service=qdrant,
                        llm_service=llm_service
                    )

                    guide_result = guide_executor.execute_all_guides(
                        project_number=project_oenum,
                        document_id=doc_id,
                        llm_mode="offline"
                    )

                    if guide_result.get("success"):
                        logger.info(f"‚úÖ Extracted {guide_result.get('extracted_count', 0)} spec values")

        return {
            "success": True,
            "doc_id": doc_id,
            "filename": file_path.name,
            "doc_type": doc_type,
            "category": category,
            "confidence": confidence,
            "chunks_indexed": chunks_indexed,
            "processing_method": "enhanced",
            "message": f"Document processed with enhanced pipeline"
        }

    except Exception as e:
        logger.error(f"‚ùå Enhanced document processing failed: {e}", exc_info=True)
        return {
            "success": False,
            "error": str(e)
        }
```

4. **Update function call** (line 223):
```python
# OLD:
result = await process_and_index_document(...)

# NEW:
result = await process_and_index_document_enhanced(
    file_path=temp_file,
    user_id=user_id,
    project_oenum=project_oenum,
    doc_processor=doc_processor,
    classifier=classifier,
    vector_rag=vector_rag,
    graph_init=graph_init,
    neo4j_service=neo4j,
    redis_service=redis,  # ‚úÖ ADD
    llm_service=llm       # ‚úÖ ADD
)
```

**Expected Impact:**
- ‚úÖ All documents get section-based processing
- ‚úÖ Summaries generated for better retrieval
- ‚úÖ Entities extracted for all project technical docs
- ‚úÖ Spec documents get full extraction guide processing
- ‚úÖ Document tracking in Redis for context

---

## üü† P1: INITIALIZE EXTRACTION GUIDES ON PROJECT CREATION

### Current Problem
Extraction guides are only loaded during spec extraction, not when project is created.

### Solution

**File:** `/simorgh-agent/backend/main.py`

**Location:** After line 415 (in `POST /api/projects` endpoint)

**Add after successful project creation:**

```python
@app.post("/api/projects", response_model=dict)
async def create_project(
    request: Request,
    current_user: str = Depends(get_current_user),
    neo4j: Neo4jService = Depends(get_neo4j)
):
    # ... existing code ...

    # After line 415: graph_init.initialize_project_structure(...)

    # ‚úÖ ADD THIS: Initialize extraction guides for the project
    logger.info(f"üìã Initializing extraction guides for project {project_number}")

    try:
        from services.document_processing_integration import initialize_project_guides

        guides_initialized = initialize_project_guides(
            neo4j_driver=neo4j.driver,
            project_number=project_number
        )

        if guides_initialized:
            logger.info(f"‚úÖ Initialized {guides_initialized} extraction guides")
        else:
            logger.warning(f"‚ö†Ô∏è Failed to initialize extraction guides")
    except Exception as e:
        logger.error(f"‚ùå Guide initialization error: {e}")
        # Don't fail project creation if guides fail

    # ... rest of endpoint ...
```

**Expected Impact:**
- ‚úÖ All 60 extraction guides pre-loaded on project creation
- ‚úÖ Faster spec extraction (guides already in graph)
- ‚úÖ Guides available for manual extraction queries

---

## üü† P1: EXPOSE GRAPH QUERY ENDPOINTS TO FRONTEND

### Current Problem
Powerful graph query endpoints exist but aren't in frontend API client.

### Solution

**File:** `/simorgh-agent/frontend/src/api/api.ts` (or equivalent)

**Add these endpoint wrappers:**

```typescript
// Graph Query APIs
export const graphAPI = {
  // Semantic search in knowledge graph
  searchGraph: async (projectNumber: string, query: string, limit: number = 5) => {
    const response = await api.post('/api/graph/query', {
      project_number: projectNumber,
      query: query,
      limit: limit
    });
    return response.data;
  },

  // Get entity with its relationships
  getEntity: async (projectNumber: string, entityId: string, depth: number = 2) => {
    const response = await api.get(`/api/graph/${projectNumber}/entity/${entityId}`, {
      params: { depth }
    });
    return response.data;
  },

  // Find power flow paths
  findPowerPath: async (projectNumber: string, sourceId: string, targetId: string) => {
    const response = await api.post(`/api/graph/${projectNumber}/power-path`, {
      source_entity_id: sourceId,
      target_entity_id: targetId
    });
    return response.data;
  },

  // Get all extracted specifications
  getSpecifications: async (projectNumber: string) => {
    const response = await api.get(`/api/projects/${projectNumber}/specifications/summary`);
    return response.data;
  }
};
```

**Create new frontend component:**

```tsx
// components/GraphViewer.tsx
import { graphAPI } from '@/api/api';

export function GraphViewer({ projectNumber }: { projectNumber: string }) {
  const [entities, setEntities] = useState([]);
  const [query, setQuery] = useState('');

  const handleSearch = async () => {
    const results = await graphAPI.searchGraph(projectNumber, query);
    setEntities(results.entities);
  };

  return (
    <div className="graph-viewer">
      <input
        value={query}
        onChange={(e) => setQuery(e.target.value)}
        placeholder="Search knowledge graph..."
      />
      <button onClick={handleSearch}>Search</button>

      <div className="results">
        {entities.map(entity => (
          <EntityCard key={entity.id} entity={entity} />
        ))}
      </div>
    </div>
  );
}
```

**Expected Impact:**
- ‚úÖ Users can browse extracted entities
- ‚úÖ Visualize equipment relationships
- ‚úÖ Trace power distribution paths
- ‚úÖ View all extracted specifications

---

## üü° P2: ADD ELECTRICAL KNOWLEDGE BASE TO LLM PROMPTS

### Current Problem
No anthology or base knowledge is injected into LLM context.

### Solution

**Step 1: Create Knowledge Base File**

**File:** `/simorgh-agent/backend/knowledge/electrical_anthology.py`

```python
"""
Electrical Systems Knowledge Base
==================================
Standard knowledge about electrical systems, IEC standards, and common specifications.
"""

ELECTRICAL_KNOWLEDGE_BASE = """
## Electrical Systems Fundamentals

### Voltage Levels (IEC 60038)
- **Extra Low Voltage (ELV)**: ‚â§ 50V AC or ‚â§ 120V DC
- **Low Voltage (LV)**: 50V - 1000V AC or 120V - 1500V DC
- **Medium Voltage (MV)**: 1kV - 35kV
- **High Voltage (HV)**: > 35kV

### Standard Voltages (Iran/IEC)
- **LV Distribution**: 230V (phase), 400V (3-phase)
- **MV Distribution**: 6.6kV, 11kV, 20kV, 33kV
- **HV Transmission**: 63kV, 132kV, 230kV, 400kV

### Switchgear Types
- **Air Insulated Switchgear (AIS)**: Traditional, outdoor
- **Gas Insulated Switchgear (GIS)**: Compact, SF6 gas, indoor
- **Metal-Clad Switchgear**: Enclosed, withdrawable circuit breakers
- **Metal-Enclosed Switchgear**: Fixed equipment, compartmentalized

### Circuit Breaker Technologies
- **Vacuum Circuit Breakers (VCB)**: MV applications, 3.6kV - 40.5kV
- **SF6 Circuit Breakers**: MV/HV, excellent arc quenching
- **Air Circuit Breakers (ACB)**: LV applications, up to 6300A
- **Molded Case Circuit Breakers (MCCB)**: LV, up to 2500A

### Protection Relay Types (IEC 60255)
- **Overcurrent (50/51)**: Instantaneous/time-delayed
- **Differential (87)**: Transformers, generators, busbars
- **Distance (21)**: Transmission lines
- **Under/Over Voltage (27/59)**
- **Under/Over Frequency (81)**

### IP Ratings (IEC 60529)
- **IP42**: Indoor panels (dust, dripping water)
- **IP54**: Outdoor panels (dust, splashing water)
- **IP65**: Dust-tight, jet-proof

### Short Circuit Calculations
- **Breaking Capacity**: kA rating for circuit breaker
- **Short-Time Withstand**: 1s or 3s rating
- **Peak Withstand**: 2.5 √ó RMS current

### Busbar Design
- **Material**: Copper (Cu) or Aluminum (Al)
- **Rating**: Based on current density (A/mm¬≤)
- **Typical**: 0.8 - 1.5 A/mm¬≤ for Cu

### Common Standards
- **IEC 60947**: Low-voltage switchgear and controlgear
- **IEC 62271**: High-voltage switchgear and controlgear
- **IEC 60076**: Power transformers
- **IEC 60909**: Short-circuit currents
- **IEEE 1584**: Arc flash hazard calculation
"""

COMMON_ABBREVIATIONS = {
    "ACB": "Air Circuit Breaker",
    "MCCB": "Molded Case Circuit Breaker",
    "VCB": "Vacuum Circuit Breaker",
    "SF6": "Sulfur Hexafluoride",
    "CT": "Current Transformer",
    "VT/PT": "Voltage Transformer / Potential Transformer",
    "MV": "Medium Voltage",
    "LV": "Low Voltage",
    "HV": "High Voltage",
    "kA": "kilo-Ampere (short circuit current)",
    "MVA": "Mega Volt-Ampere (apparent power)",
    "IP": "Ingress Protection (enclosure rating)",
    "IEC": "International Electrotechnical Commission",
    "IEEE": "Institute of Electrical and Electronics Engineers"
}

def get_knowledge_context() -> str:
    """Get electrical knowledge base for LLM context"""
    return ELECTRICAL_KNOWLEDGE_BASE
```

**Step 2: Inject into LLM Prompts**

**File:** `/simorgh-agent/backend/services/llm_service.py`

**Modify `generate()` method** (around line 150):

```python
def generate(
    self,
    messages: List[Dict[str, str]],
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    mode: Optional[str] = None,
    inject_knowledge: bool = True  # ‚úÖ ADD THIS
) -> Dict[str, Any]:
    """
    Generate LLM response

    Args:
        inject_knowledge: If True, inject electrical knowledge base into system prompt
    """

    # ‚úÖ ADD: Inject knowledge base
    if inject_knowledge:
        from knowledge.electrical_anthology import get_knowledge_context

        knowledge = get_knowledge_context()

        # Find system message and append knowledge
        for msg in messages:
            if msg.get("role") == "system":
                msg["content"] += f"\n\n## Reference Knowledge Base\n{knowledge}"
                break
        else:
            # No system message, add one
            messages.insert(0, {
                "role": "system",
                "content": f"You are Simorgh, an expert electrical engineering assistant.\n\n## Reference Knowledge Base\n{knowledge}"
            })

    # ... rest of method unchanged ...
```

**Expected Impact:**
- ‚úÖ LLM has base electrical knowledge
- ‚úÖ Better understanding of technical terms
- ‚úÖ More accurate answers about standards
- ‚úÖ Proper interpretation of voltage levels, ratings

---

## üü° P2: INTEGRATE COCOINDEX INTO ENHANCED PIPELINE

### Current Problem
CocoIndex is a separate endpoint, not part of main document processing flow.

### Solution

**File:** `/simorgh-agent/backend/routes/documents_rag.py`

**Add after entity extraction** (in the enhanced function above):

```python
# After GraphBuilder entity extraction (around line 190 in enhanced version)

# ‚úÖ ADD: Try CocoIndex for PDF documents if available
if file_path.suffix.lower() == '.pdf':
    logger.info(f"üìÑ Attempting CocoIndex extraction for PDF")

    try:
        from cocoindex_flows.industrial_electrical_flow import process_document as cocoindex_process

        cocoindex_result = await cocoindex_process(
            file_path=str(file_path),
            project_number=project_oenum,
            user_id=user_id
        )

        if cocoindex_result.get("success"):
            logger.info(f"‚úÖ CocoIndex extracted {cocoindex_result.get('entities_extracted', 0)} entities")

            # Merge with GraphBuilder entities (deduplicate by name/type)
            # TODO: Implement entity merging logic
        else:
            logger.warning(f"‚ö†Ô∏è CocoIndex extraction failed: {cocoindex_result.get('error')}")

    except ImportError:
        logger.warning("‚ö†Ô∏è CocoIndex not available, skipping")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è CocoIndex error: {e}")
        # Don't fail main pipeline if CocoIndex fails
```

**Expected Impact:**
- ‚úÖ Better entity extraction for PDFs
- ‚úÖ Siemens ontology validation
- ‚úÖ Relationship extraction (power flow, connections)
- ‚úÖ Graceful fallback if CocoIndex unavailable

---

## üü¢ P3: CREATE SPEC REVIEW INTERFACE

### Current Problem
Extracted specs stored in Neo4j but no way to review/correct them.

### Solution

**File:** `/simorgh-agent/backend/main.py`

**Add new endpoint after line 2497:**

```python
@app.get("/api/projects/{project_number}/documents/{document_id}/specs/review")
async def get_specs_for_review(
    project_number: str,
    document_id: str,
    current_user: str = Depends(get_current_user),
    neo4j: Neo4jService = Depends(get_neo4j)
):
    """
    Get extracted specifications for human review
    Returns all ActualValue nodes for the document with their confidence scores
    """

    query = """
    MATCH (d:Document {document_id: $document_id})
          -[:HAS_SPECIFICATION]->(spec:SpecCategory)
          -[:HAS_FIELD]->(field:SpecField)
          -[:HAS_VALUE]->(value:ActualValue)
    WHERE d.project_number = $project_number
    RETURN spec.name as category,
           field.name as field_name,
           field.definition as field_definition,
           value.value as extracted_value,
           value.confidence as confidence,
           value.source_section as source_section,
           value.reviewed as reviewed,
           value.reviewed_by as reviewed_by,
           value.corrected_value as corrected_value,
           ID(value) as value_id
    ORDER BY spec.name, field.name
    """

    with neo4j.driver.session() as session:
        result = session.run(query, {
            "project_number": project_number,
            "document_id": document_id
        })

        specs = []
        for record in result:
            specs.append({
                "category": record["category"],
                "field_name": record["field_name"],
                "field_definition": record["field_definition"],
                "extracted_value": record["extracted_value"],
                "confidence": record["confidence"],
                "source_section": record["source_section"],
                "reviewed": record["reviewed"] or False,
                "reviewed_by": record["reviewed_by"],
                "corrected_value": record["corrected_value"],
                "value_id": record["value_id"]
            })

    return {
        "success": True,
        "project_number": project_number,
        "document_id": document_id,
        "specifications": specs,
        "total_count": len(specs)
    }


@app.post("/api/projects/{project_number}/specs/review")
async def submit_spec_review(
    project_number: str,
    request: Request,
    current_user: str = Depends(get_current_user),
    neo4j: Neo4jService = Depends(get_neo4j)
):
    """
    Submit reviewed/corrected specification values
    """
    body = await request.json()
    value_id = body.get("value_id")
    corrected_value = body.get("corrected_value")

    query = """
    MATCH (value:ActualValue)
    WHERE ID(value) = $value_id
    SET value.reviewed = true,
        value.reviewed_by = $user_id,
        value.reviewed_at = datetime(),
        value.corrected_value = $corrected_value
    RETURN value
    """

    with neo4j.driver.session() as session:
        session.run(query, {
            "value_id": value_id,
            "user_id": current_user,
            "corrected_value": corrected_value
        })

    return {
        "success": True,
        "message": "Specification reviewed successfully"
    }
```

**Frontend Component:**

```tsx
// components/SpecReview.tsx
export function SpecReviewInterface({ projectNumber, documentId }) {
  const [specs, setSpecs] = useState([]);

  useEffect(() => {
    // Fetch specs for review
    api.get(`/api/projects/${projectNumber}/documents/${documentId}/specs/review`)
      .then(res => setSpecs(res.data.specifications));
  }, [projectNumber, documentId]);

  const handleCorrection = async (valueId, correctedValue) => {
    await api.post(`/api/projects/${projectNumber}/specs/review`, {
      value_id: valueId,
      corrected_value: correctedValue
    });
    // Refresh
  };

  return (
    <table>
      <thead>
        <tr>
          <th>Category</th>
          <th>Field</th>
          <th>Extracted Value</th>
          <th>Confidence</th>
          <th>Correction</th>
        </tr>
      </thead>
      <tbody>
        {specs.map(spec => (
          <tr key={spec.value_id}>
            <td>{spec.category}</td>
            <td>{spec.field_name}</td>
            <td>{spec.extracted_value}</td>
            <td>{(spec.confidence * 100).toFixed(0)}%</td>
            <td>
              <input
                defaultValue={spec.corrected_value || spec.extracted_value}
                onBlur={(e) => handleCorrection(spec.value_id, e.target.value)}
              />
            </td>
          </tr>
        ))}
      </tbody>
    </table>
  );
}
```

**Expected Impact:**
- ‚úÖ Human-in-the-loop for spec extraction
- ‚úÖ Quality assurance for critical values
- ‚úÖ Feedback loop for ML improvement
- ‚úÖ Audit trail of corrections

---

## üìã IMPLEMENTATION SEQUENCE

### Phase 1: Core Enhancements (Week 1)
1. ‚úÖ Integrate enhanced pipeline into main upload (**P0**)
2. ‚úÖ Initialize extraction guides on project creation (**P1**)
3. ‚úÖ Add electrical knowledge base to LLM (**P2**)

### Phase 2: Frontend Integration (Week 2)
4. ‚úÖ Expose graph query endpoints to frontend (**P1**)
5. ‚úÖ Create graph viewer component
6. ‚úÖ Add spec summary view

### Phase 3: Advanced Features (Week 3)
7. ‚úÖ Integrate CocoIndex into pipeline (**P2**)
8. ‚úÖ Create spec review interface (**P3**)
9. ‚úÖ Add entity merging logic

---

## üéÅ BONUS ENHANCEMENTS

### Auto-Generate Electrical Single-Line Diagrams
Use extracted entities to create visual diagrams showing:
- Power distribution hierarchy
- Protection scheme
- Equipment connections

### Intelligent Spec Comparison
Compare uploaded specs against:
- IEC standard requirements
- Previous project specifications
- Industry best practices

### Predictive Entity Extraction
Use extraction history to improve:
- Field prediction accuracy
- Entity relationship inference
- Automatic validation rules

---

## üìä EXPECTED OUTCOMES

**Before Enhancements:**
- Basic text chunking
- No entity extraction for most documents
- Extraction guides unused
- Graph features hidden
- Manual spec review impossible

**After Enhancements:**
- Section-based semantic search
- Entity extraction for all technical docs
- 60 spec fields automatically extracted
- Graph visualization available
- Human-in-the-loop quality assurance
- Electrical knowledge in LLM context

**Estimated Quality Improvement:**
- **Answer Accuracy**: 40% ‚Üí 85%
- **Spec Extraction**: 0% ‚Üí 75%
- **Entity Coverage**: 10% ‚Üí 90%
- **User Satisfaction**: 3/5 ‚Üí 4.5/5

---

## üöÄ NEXT STEPS

1. **Review this plan** with your team
2. **Test local LLM** (fix the JSON response issue first)
3. **Implement P0** (enhanced pipeline integration)
4. **Test with real documents** (spec PDFs)
5. **Iterate** based on results
6. **Deploy** incrementally

---

**Questions? Let's discuss implementation details for any specific enhancement!**
